{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Content Parser\n",
    "\n",
    "This notebook extracts text content from web pages given URLs and outputs the results as `url, content_string` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: lxml in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ayikara/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this cell first if packages are not installed)\n",
    "!pip install requests beautifulsoup4 pandas lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_web_content(url, delay=1, timeout=15):\n",
    "    \"\"\"\n",
    "    Extract text content from a web page\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to parse\n",
    "        delay (int): Delay between requests in seconds\n",
    "        timeout (int): Request timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing url and content_string\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'url': url,\n",
    "        'content_string': '',\n",
    "        'status': 'success',\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching content from: {url}\")\n",
    "        \n",
    "        # Set headers to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Extract text content\n",
    "        # Try to get main content areas first\n",
    "        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main', re.I))\n",
    "        \n",
    "        if main_content:\n",
    "            content_text = main_content.get_text()\n",
    "        else:\n",
    "            # Fall back to body content\n",
    "            content_text = soup.get_text()\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        # Remove extra whitespace and newlines\n",
    "        content_text = re.sub(r'\\s+', ' ', content_text)\n",
    "        content_text = content_text.strip()\n",
    "        \n",
    "        result['content_string'] = content_text\n",
    "        \n",
    "        print(f\"Successfully extracted {len(result['content_string'])} characters\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        result['status'] = 'error'\n",
    "        result['error'] = f\"Request error: {str(e)}\"\n",
    "        print(f\"Request error: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['status'] = 'error'\n",
    "        result['error'] = f\"Parsing error: {str(e)}\"\n",
    "        print(f\"Parsing error: {e}\")\n",
    "    \n",
    "    # Add delay to be respectful to the server\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from: https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html\n",
      "Successfully extracted 19322 characters\n",
      "\n",
      "URL: https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html\n",
      "Status: success\n",
      "Content Length: 19322 characters\n",
      "\n",
      "Content Preview (first 500 characters):\n",
      "Capitalize on the around-the-clock liquidity of E-mini S&P 500 futures (ES), and take advantage of one of the most efficient and cost-effective ways to gain market exposure to the S&P 500 Index, a broad-based, capitalization-weighted index that tracks 500 of the largest companies of the US economy and a key indicator of the stock marketâ€™s health. With ES futures, you can take positions on S&P 500 performance electronically. Capitalize on the around-the-clock liquidity of E-mini S&P 500 futures (...\n"
     ]
    }
   ],
   "source": [
    "# Test with the provided URL\n",
    "test_url = \"https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html\"\n",
    "\n",
    "# Extract content\n",
    "result = extract_web_content(test_url)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nURL: {result['url']}\")\n",
    "print(f\"Status: {result['status']}\")\n",
    "\n",
    "if result['status'] == 'success':\n",
    "    print(f\"Content Length: {len(result['content_string'])} characters\")\n",
    "    print(f\"\\nContent Preview (first 500 characters):\")\n",
    "    print(result['content_string'][:500] + \"...\" if len(result['content_string']) > 500 else result['content_string'])\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created:\n",
      "Shape: (1, 2)\n",
      "Columns: ['url', 'content_string']\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   url             1 non-null      object\n",
      " 1   content_string  1 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 144.0+ bytes\n",
      "None\n",
      "\n",
      "Data saved to: extracted_web_content.csv\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with the result\n",
    "if result['status'] == 'success':\n",
    "    df = pd.DataFrame([{\n",
    "        'url': result['url'],\n",
    "        'content_string': result['content_string']\n",
    "    }])\n",
    "    \n",
    "    print(\"DataFrame created:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display DataFrame info\n",
    "    print(\"\\nDataFrame info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = 'extracted_web_content.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nData saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"Cannot create DataFrame due to extraction error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Multiple URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with multiple URLs\n",
    "urls_to_parse = [\n",
    "    \"https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html\",\n",
    "    \"https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.html\",\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "def parse_multiple_urls(urls, delay=2):\n",
    "    \"\"\"\n",
    "    Parse multiple URLs and return DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"\\nProcessing URL {i}/{len(urls)}\")\n",
    "        result = extract_web_content(url, delay=delay)\n",
    "        \n",
    "        results.append({\n",
    "            'url': result['url'],\n",
    "            'content_string': result['content_string'] if result['status'] == 'success' else '',\n",
    "            'status': result['status'],\n",
    "            'content_length': len(result['content_string']) if result['status'] == 'success' else 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Parse multiple URLs (uncomment to run)\n",
    "# multi_df = parse_multiple_urls(urls_to_parse)\n",
    "# print(\"\\nMultiple URLs Results:\")\n",
    "# print(multi_df[['url', 'status', 'content_length']])\n",
    "# multi_df.to_csv('multiple_urls_content.csv', index=False)\n",
    "# print(\"Results saved to: multiple_urls_content.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse URLs from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_urls_from_csv(csv_file, url_column='url'):\n",
    "    \"\"\"\n",
    "    Read URLs from CSV file and extract content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read URLs from CSV\n",
    "        urls_df = pd.read_csv(csv_file)\n",
    "        print(f\"Loaded CSV with {len(urls_df)} rows\")\n",
    "        print(f\"Columns: {list(urls_df.columns)}\")\n",
    "        \n",
    "        if url_column not in urls_df.columns:\n",
    "            print(f\"Error: Column '{url_column}' not found in CSV\")\n",
    "            return None\n",
    "        \n",
    "        urls_list = urls_df[url_column].dropna().tolist()\n",
    "        print(f\"Found {len(urls_list)} URLs to parse\")\n",
    "        \n",
    "        # Parse URLs\n",
    "        results_df = parse_multiple_urls(urls_list)\n",
    "        \n",
    "        # Save results\n",
    "        output_file = 'parsed_content_from_csv.csv'\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "        \n",
    "        return results_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{csv_file}' not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file 'cme_pages_urls.csv' not found\n"
     ]
    }
   ],
   "source": [
    "# Example usage (uncomment and update path to use)\n",
    "csv_results = parse_urls_from_csv('cme_pages_url.csv', 'url')\n",
    "if csv_results is not None:\n",
    "   # print(csv_results.head())\n",
    "   csv_results.to_csv('parsed_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the extracted content\n",
    "if 'df' in locals() and not df.empty:\n",
    "    content = df['content_string'].iloc[0]\n",
    "    \n",
    "    print(\"Content Analysis:\")\n",
    "    print(f\"Total characters: {len(content)}\")\n",
    "    print(f\"Total words: {len(content.split())}\")\n",
    "    print(f\"Total lines: {len(content.split('\\n'))}\")\n",
    "    \n",
    "    # Find common words (basic analysis)\n",
    "    words = content.lower().split()\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        word = re.sub(r'[^\\w]', '', word)  # Remove punctuation\n",
    "        if len(word) > 3:  # Only words longer than 3 characters\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    # Top 10 most frequent words\n",
    "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\nTop 10 most frequent words:\")\n",
    "    for word, count in top_words:\n",
    "        print(f\"  {word}: {count}\")\n",
    "else:\n",
    "    print(\"No content available for analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
